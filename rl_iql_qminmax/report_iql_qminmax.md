# Συγκριτική Μελέτη Independent Q-Learning και Minimax Q-Learning σε Παίγνια Μηδενικού Αθροίσματος: Η Περίπτωση του Μετασχηματισμένου Διλήμματος του Φυλακισμένου

## 1. Εισαγωγή και Διατύπωση Προβλήματος

Στην παρούσα εργασία διερευνάται η διαδικασία μάθησης ισορροπίας σε ένα επαναλαμβανόμενο παίγνιο δύο παικτών. Συγκεκριμένα, συγκρίνονται δύο διαφορετικές προσεγγίσεις Πολυπρακτορικής Ενισχυτικής Μάθησης (Multi-Agent Reinforcement Learning - MARL):
1.  **Independent Q-Learning (IQL):** Μια "αφελής" προσέγγιση που αγνοεί την ύπαρξη του αντιπάλου.
2.  **Minimax Q-Learning:** Μια προσέγγιση ειδικά σχεδιασμένη για ανταγωνιστικά παίγνια μηδενικού αθροίσματος (zero-sum).

Το πεδίο δοκιμής είναι το **Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου (Repeated Prisoner’s Dilemma)**, το οποίο όμως έχει μετατραπεί τεχνητά σε παίγνιο μηδενικού αθροίσματος μέσω της τεχνικής **Reward Shaping**.

### Χαρακτηριστικά Περιβάλλοντος
- **Διάρκεια:** $50.000$ επεισόδια με ορίζοντα $200$ βημάτων το καθένα.
- **Χώρος Δράσεων:** $A = \{C, D\}$ (Συνεργασία, Προδοσία).
- **Ορισμός Κατάστασης:** Η κατάσταση $s_t$ καθορίζεται από την κοινή δράση του προηγούμενου γύρου:
  $$s_t = (a_A^{t-1}, a_B^{t-1})$$
  γεγονός που δημιουργεί ένα στοχαστικό παίγνιο 4 καταστάσεων.

### Zero-Sum Reward Shaping
Για να καταστεί δυνατή η εφαρμογή του αλγορίθμου Minimax, οι απολαβές τροποποιούνται ώστε το κέρδος του ενός παίκτη να ισούται με τη ζημία του άλλου:

$$
r_A^{zs} = r_A - r_B
$$
$$
r_B^{zs} = -r_A^{zs} \implies r_A^{zs} + r_B^{zs} = 0
$$

Μέσω αυτού του μετασχηματισμού, παρόλο που το υποκείμενο παίγνιο παραμένει το Prisoner's Dilemma, ο στόχος βελτιστοποίησης γίνεται αμιγώς ανταγωνιστικός.

---

## 2. Αλγόριθμοι Μάθησης

### 2.1 Independent Q-Learning (IQL)
Ο πράκτορας IQL αντιμετωπίζει τον αντίπαλο ως μέρος του στατικού περιβάλλοντος. Μαθαίνει μια συνάρτηση αξίας $Q(s,a)$ χρησιμοποιώντας τον standard κανόνα ανανέωσης Q-Learning:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \Big( r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big)
$$

Χρησιμοποιείται πολιτική **$\varepsilon$-greedy** με γραμμική μείωση της εξερεύνησης ($\varepsilon$) από $1.0$ σε $0.03$ κατά τα πρώτα $20.000$ επεισόδια.
- $\alpha = 0.1$
- $\gamma = 0.95$

### 2.2 Minimax Q-Learning
Σε περιβάλλοντα μηδενικού αθροίσματος, ο πράκτορας οφείλει να λαμβάνει υπόψη ότι ο αντίπαλος θα προσπαθήσει να ελαχιστοποιήσει το κέρδος του. Η συνάρτηση αξίας εξαρτάται από τις δράσεις και των δύο παικτών $Q(s, a_{self}, a_{opp})$.

Η αξία της κατάστασης $V(s)$ ορίζεται βάσει της αρχής του **minimax**:
$$
V(s) = \max_{a} \min_{a'} Q(s, a, a')
$$

Ο κανόνας ανανέωσης ενσωματώνει αυτή την αξία:
$$
Q(s,a_i,a_j) \leftarrow Q(s,a_i,a_j) + \alpha \Big( r + \gamma V(s') - Q(s,a_i,a_j) \Big)
$$
Η πολιτική του πράκτορα επιλέγει τη δράση που μεγιστοποιεί το ελάχιστο προσδοκώμενο κέρδος (maximin strategy).

---

## 3. Μεθοδολογία Αξιολόγησης

Η σύγκριση των αλγορίθμων βασίζεται στις ακόλουθες μετρικές (με χρήση rolling window 200 επεισοδίων):

1.  **Rolling Joint-Action Occupancy:** Η συχνότητα εμφάνισης των ζευγών δράσεων $(C,C), (D,D), (C,D), (D,C)$.
2.  **Rolling Reward Variance:** Η διακύμανση των ανταμοιβών $Var(R)$, η οποία αποτελεί δείκτη σταθερότητας της στρατηγικής.
3.  **L1 Occupancy Change:** Ο ρυθμός μεταβολής της κατανομής των καταστάσεων, που υποδεικνύει την ταχύτητα σύγκλισης:
    $$L1_t = \sum_i \left| p_t(i) - p_{t-1}(i) \right|$$
4.  **Rolling Episode Return:** Το αθροιστικό shaped reward ανά επεισόδιο. Σε ισορροπία zero-sum, αναμένεται $E[R] \approx 0$.

---

## 4. Ανάλυση Αποτελεσμάτων

### 4.1 Σύγκλιση Στρατηγικής (Occupancy)
Παρατηρείται σαφής και μονοτονική αύξηση της κατάστασης **$(D,D)$**, η οποία κυριαρχεί πλήρως (ποσοστό $\approx 100\%$) μετά το πέρας των $20.000$ επεισοδίων (περίοδος μείωσης του $\varepsilon$).

**Ερμηνεία:**
Στον μετασχηματισμένο πίνακα απολαβών, η κατάσταση $(D,D)$ αποτελεί το σημείο ισορροπίας (saddle point) με τιμή 0. Ο Minimax πράκτορας οδηγεί το παιχνίδι σε αυτή την ασφαλή στρατηγική, και ο IQL πράκτορας αναγκάζεται να προσαρμοστεί (best response) παίζοντας επίσης $D$.

### 4.2 Σταθερότητα (Variance & L1 Change)
- **L1 Change:** Εμφανίζει υψηλές τιμές μόνο στην αρχή. Μετά το επεισόδιο $20.000$, η μεταβολή μηδενίζεται, υποδεικνύοντας ότι το σύστημα έχει "κλειδώσει" σε σταθερή συμπεριφορά.
- **Reward Variance:** Υψηλή διακύμανση κατά τη φάση εξερεύνησης, η οποία μειώνεται απότομα και σταθεροποιείται σε χαμηλά επίπεδα μόλις ολοκληρωθεί η σύγκλιση.

### 4.3 Απολαβές (Return)
Οι μέσες απολαβές παρουσιάζουν αρχικά έντονες διακυμάνσεις, αλλά τελικά συγκλίνουν στο $0$. Αυτό επιβεβαιώνει ότι έχει επιτευχθεί η ισορροπία του παιγνίου μηδενικού αθροίσματος (Value of the Game = 0).

---

## 5. Συμπεράσματα

Η πειραματική διαδικασία κατέδειξε τα εξής:

1.  **Επιτυχής Σύγκλιση:** Σε αντίθεση με την περίπτωση του απλού IQL vs IQL, η εισαγωγή του Minimax πράκτορα και η zero-sum δομή οδήγησαν σε ταχεία και σταθερή σύγκλιση.
2.  **Χρόνος Σύγκλισης:** Το σύστημα σταθεροποιείται περίπου στα $20.000$ επεισόδια, ταυτιζόμενο χρονικά με το τέλος της φάσης έντονης εξερεύνησης (exploration decay).
3.  **Κυριαρχία της Στρατηγικής Minimax:** Ο αλγόριθμος Minimax Q-Learning επέβαλε επιτυχώς τη στρατηγική ασφαλείας $(D,D)$, μετατρέποντας ένα κοινωνικό δίλημμα σε καθαρά ανταγωνιστική ισορροπία.
# Μελέτη της Δυναμικής Μάθησης Ανεξάρτητων Πρακτόρων (Independent Q-Learning) στο Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου

## 1. Εισαγωγή και Περιγραφή Προβλήματος

Στην παρούσα ενότητα εξετάζεται η συμπεριφορά δύο πρακτόρων Ενισχυτικής Μάθησης (Reinforcement Learning - RL) που αλληλεπιδρούν σε ένα περιβάλλον **Επαναλαμβανόμενου Διλήμματος του Φυλακισμένου (Repeated Prisoner’s Dilemma - RPD)**.

Το πρόβλημα χαρακτηρίζεται από τα εξής:
- **Πράκτορες:** 2 ανεξάρτητοι μάθοι (Independent Learners), οι οποίοι δεν έχουν άμεση γνώση της πολιτικής του αντιπάλου, αλλά τον αντιμετωπίζουν ως μέρος ενός μη-στατικού περιβάλλοντος.
- **Διάρκεια:** Το πείραμα εκτελείται για $50.000$ επεισόδια, με ορίζοντα $200$ βημάτων ανά επεισόδιο.
- **Χώρος Καταστάσεων:** Η κατάσταση $s_t$ ορίζεται από την κοινή δράση του προηγούμενου γύρου: $s_t \in \{CC, CD, DC, DD\}$.
- **Χώρος Δράσεων:** $A = \{C, D\}$, όπου $C$ η Συνεργασία (Cooperate) και $D$ η Προδοσία (Defect).

Στόχος είναι η διερεύνηση της σύγκλισης του συστήματος: αν οδηγείται στο Nash Equilibrium $(D,D)$, σε αμοιβαία συνεργασία $(C,C)$, ή σε ταλαντωτική συμπεριφορά.

---

## 2. Θεωρητικό Υπόβαθρο

### 2.1 Το Δίλημμα του Φυλακισμένου

Ο πίνακας αποδόσεων (payoff matrix) του παιγνίου ορίζεται ως εξής:

|        | **C** | **D** |
|--------|----------|----------|
| **C** | $(3,3)$  | $(0,5)$  |
| **D** | $(5,0)$  | $(-1,-1)$|

Η στρατηγική $D$ είναι αυστηρά κυρίαρχη (strictly dominant), καθιστώντας το προφίλ $(D,D)$ το μοναδικό **Ισορροπία Nash (Nash Equilibrium)**, παρόλο που το $(C,C)$ είναι Pareto optimal.

### 2.2 Αλγόριθμος Independent Q-Learning (IQL)

Κάθε πράκτορας διατηρεί έναν πίνακα τιμών $Q(s,a)$ και τον ενημερώνει ανεξάρτητα, αγνοώντας τη διαδικασία μάθησης του αντιπάλου. Ο κανόνας ενημέρωσης είναι:

$$
Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') \right]
$$

**Παράμετροι Μάθησης:**
- **Learning Rate:** $\alpha = 0.1$
- **Discount Factor:** $\gamma = 0.95$ (ενθαρρύνει μακροπρόθεσμες στρατηγικές)

**Πολιτική Εξερεύνησης ($\varepsilon$-greedy):**
Η πιθανότητα τυχαίας δράσης ($\varepsilon$) μειώνεται γραμμικά από $\varepsilon_{start} = 1.0$ σε $\varepsilon_{end} = 0.03$ κατά τα πρώτα $20.000$ επεισόδια. Στη συνέχεια παραμένει σταθερή στο $0.03$ (97% greedy policy).

---

## 3. Μεθοδολογία Αξιολόγησης

Για την ανάλυση της δυναμικής του συστήματος χρησιμοποιούνται οι εξής μετρικές (υπολογισμένες με κινούμενο παράθυρο 200 επεισοδίων):

### 3.1 Απόδοση και Στρατηγική
- **Rolling Mean Episode Return:** Η μέση συνολική ανταμοιβή ανά επεισόδιο.
- **Πιθανότητα Συνεργασίας $P(C)$:** Η συχνότητα επιλογής της δράσης $C$.
- **Joint Action Occupancy:** Το ποσοστό εμφάνισης των καταστάσεων $(C,C), (D,D), (C,D), (D,C)$.

### 3.2 Σταθερότητα (Stability)
- **Action Switch Rate:** Η συχνότητα αλλαγής δράσης εντός του επεισοδίου.
- **L1 Occupancy Change:** Ο ρυθμός μεταβολής της κατανομής των καταστάσεων:
  $$L1_t = \sum_i | occ_t^i - occ_{t-1}^i |$$
- **Rolling Reward Variance:** Η διακύμανση των ανταμοιβών στο παράθυρο παρατήρησης:
  $$Var(R) = \frac{1}{N} \sum_{i=1}^{N} (R_i - \bar{R})^2$$

---

## 4. Πειραματικά Αποτελέσματα

Η ανάλυση των αποτελεσμάτων διακρίνεται σε δύο φάσεις:

### Φάση 1: Εκμάθηση (0 – 20.000 επεισόδια)
Κατά τη διάρκεια της μείωσης του $\varepsilon$:
- Η πιθανότητα συνεργασίας μειώνεται σταδιακά.
- Το σύστημα τείνει προς το Nash Equilibrium $(D,D)$.
- Οι αποδόσεις των πρακτόρων μειώνονται, προσεγγίζοντας την τιμή της αμοιβαίας προδοσίας.

### Φάση 2: Ώριμη Λειτουργία (> 20.000 επεισόδια)
Με σταθερό $\varepsilon = 0.03$, παρατηρούνται τα εξής φαινόμενα:
- **Αστάθεια:** Δεν επιτυγχάνεται σταθερή σύγκλιση.
- **Ταλαντώσεις:** Εμφανίζονται κύκλοι όπου το σύστημα μεταβαίνει από το $(D,D)$ προσπάθειες συνεργασίας $(C,C)$ και πίσω, λόγω της προσπάθειας των πρακτόρων να μεγιστοποιήσουν το ατομικό κέρδος σε ένα περιβάλλον που αντιλαμβάνονται λανθασμένα ως στατικό.
- **Υψηλή Διακύμανση:** Το switch rate και το variance των ανταμοιβών παραμένουν υψηλά.

---

## 5. Συμπεράσματα

Η εφαρμογή του Independent Q-Learning στο Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου οδηγεί στα εξής συμπεράσματα:

1.  **Απουσία Σύγκλισης Nash:** Το σύστημα δεν καταλήγει σε σταθερή ισορροπία Nash $(D,D)$, ούτε σε μόνιμη συνεργασία.
2.  **Μη-Στατικότητα:** Η ταυτόχρονη μάθηση παραβιάζει την υπόθεση στατικότητας του Q-Learning (Markov Property), οδηγώντας σε κυκλική δυναμική.
3.  **Ευαισθησία στο Θόρυβο:** Ακόμη και ελάχιστη εξερεύνηση ($\varepsilon = 0.03$) είναι αρκετή για να διαταράξει μια προσωρινή ισορροπία και να προκαλέσει μετάβαση σε άλλο καθεστώς συμπεριφοράς (regime switching).

Συνοψίζοντας, η πολυπρακτορική μάθηση με ανεξάρτητους Q-Learners σε κοινωνικά διλήμματα τείνει να παράγει δυναμικές συμπεριφορές αντί για στατικές ισορροπίες.
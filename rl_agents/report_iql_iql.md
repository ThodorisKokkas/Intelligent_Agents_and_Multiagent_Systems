# 1. Περιγραφή Προβλήματος

Μελετάται η δυναμική μάθησης δύο ανεξάρτητων πρακτόρων Reinforcement Learning σε επαναλαμβανόμενο παιχνίδι Prisoner’s Dilemma (PD).

Χαρακτηριστικά:

- 2 παίκτες
- 50.000 επεισόδια
- 200 βήματα ανά επεισόδιο
- Κατάσταση: προηγούμενη κοινή δράση (CC, CD, DC, DD)
- Δράσεις: C (συνεργασία), D (προδοσία)

Κάθε πράκτορας θεωρεί τον αντίπαλο ως μέρος του περιβάλλοντος (μη-στατικό περιβάλλον).

Στόχος είναι να εξεταστεί αν η μάθηση οδηγεί σε:
- Nash equilibrium (D,D)
- Συνεργασία (C,C)
- Ταλαντωτική συμπεριφορά

---

# 2. Θεωρητικό Υπόβαθρο

## 2.1 Prisoner’s Dilemma

Πίνακας αποδόσεων:

|        | C        | D        |
|--------|----------|----------|
| **C**  | (3,3)    | (0,5)    |
| **D**  | (5,0)    | (-1,-1)  |

Η προδοσία (D) είναι αυστηρά κυρίαρχη στρατηγική.

Το μοναδικό Nash equilibrium είναι το (D,D).

Ορισμός Nash:

Ένα προφίλ στρατηγικών είναι Nash equilibrium αν κανένας παίκτης δεν μπορεί να αυξήσει το payoff του αλλάζοντας μονομερώς στρατηγική.

---

## 2.2 Independent Q-Learning

Κάθε πράκτορας μαθαίνει πίνακα τιμών \( Q(s,a) \).

Ο κανόνας ενημέρωσης είναι:

$$
Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') \right]
$$

Όπου:

- \( \alpha = 0.1 \) (learning rate)
- \( \gamma = 0.95 \) (discount factor)
- \( r \) η άμεση ανταμοιβή
- \( s' \) η επόμενη κατάσταση

### Ερμηνεία παραμέτρων

**Discount factor \( \gamma = 0.95 \)**  
Ορίζει πόσο σημαντικά είναι τα μελλοντικά rewards.  
Τιμή κοντά στο 1 σημαίνει μακροπρόθεσμη στρατηγική.

**ε-greedy πολιτική**

Ο πράκτορας επιλέγει:

- Με πιθανότητα \( \varepsilon \) → τυχαία δράση (exploration)
- Με πιθανότητα \( 1-\varepsilon \) → δράση με μέγιστο Q (greedy)

Ρυθμίσεις:

- \( \varepsilon_{start} = 1.0 \)
- \( \varepsilon_{end} = 0.03 \)
- Γραμμική μείωση στα πρώτα 20.000 επεισόδια

Μετά τα 20.000 επεισόδια:

$$
\varepsilon = 0.03
$$

Άρα οι πράκτορες δρουν 97% greedy.

---

# 3. Ρύθμιση Πειράματος

- 2 Independent Q-Learners
- Καμία άμεση γνώση της πολιτικής του αντιπάλου
- Rolling window 200 επεισοδίων για ομαλοποίηση

Το περιβάλλον είναι μη-στατικό, επειδή και οι δύο πράκτορες μαθαίνουν ταυτόχρονα.

---

# 4. Μετρικές

## 4.1 Απόδοση

- Rolling mean episode return
- Διαφορά ανταμοιβής \( R_A - R_B \)

## 4.2 Συμπεριφορά Στρατηγικής

- Rolling πιθανότητα συνεργασίας \( P(C) \)
- Joint action occupancy:
  - (C,C)
  - (D,D)
  - (C,D)
  - (D,C)

## 4.3 Σταθερότητα

**Action switch rate**

$$
\text{Switch Rate} = \frac{\text{Αλλαγές δράσης}}{\text{Βήματα επεισοδίου}}
$$

Μετρά πόσο συχνά αλλάζει δράση ο πράκτορας.

**L1 αλλαγή occupancy**

$$
L1_t = \sum_i | occ_t^i - occ_{t-1}^i |
$$

Μετρά πόσο αλλάζει η συνολική κατανομή στρατηγικών.

**Rolling reward variance**

$$
Var(R) = \frac{1}{N} \sum_{i=1}^{N} (R_i - \bar{R})^2
$$

όπου:

$$
\bar{R} = \frac{1}{N} \sum_{i=1}^{N} R_i
$$

είναι ο μέσος όρος των returns στο παράθυρο.

---

# 5. Πειραματικά Αποτελέσματα

### Φάση 1 (0–20.000 επεισόδια)

- Η πιθανότητα συνεργασίας μειώνεται.
- Αυξάνεται η εμφάνιση (D,D).
- Οι αποδόσεις τείνουν προς το Nash equilibrium.

### Φάση 2 (>20.000 επεισόδια)

- Παρατηρούνται έντονες ταλαντώσεις.
- Εναλλαγή μεταξύ (C,C) και (D,D).
- Υψηλή διακύμανση ανταμοιβών.
- Μη σταθερό switch rate.

Δεν ανιχνεύεται σταθερή σύγκλιση.

---

# 6. Συμπεράσματα

1. Το Independent Q-Learning δεν συγκλίνει σε σταθερό Nash equilibrium.
2. Δεν επιτυγχάνεται μόνιμη συνεργασία.
3. Παρατηρείται επίμονη ταλαντωτική δυναμική.
4. Η ταυτόχρονη μάθηση δημιουργεί μη-στατικότητα.
5. Ακόμη και μικρό exploration (\( \varepsilon = 0.03 \)) προκαλεί regime switching.

Η πολυπρακτορική μάθηση μπορεί να οδηγήσει σε δυναμική συμπεριφορά αντί για στατική ισορροπία.
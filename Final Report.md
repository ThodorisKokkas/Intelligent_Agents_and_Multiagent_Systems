# Μελέτη της Μεθόδου Fictitious Play σε Παίγνια Μηδενικού Αθροίσματος: Η Περίπτωση του Rock–Paper–Scissors

## 1. Εισαγωγή και Περιγραφή Προβλήματος

Στην παρούσα εργασία μελετάται η συμπεριφορά και η σύγκλιση της μεθόδου **Fictitious Play (FP)** στο κλασικό παιχνίδι **Rock–Paper–Scissors (RPS)**. Το RPS μοντελοποιείται ως ένα επαναλαμβανόμενο παιχνίδι (repeated game) με τα εξής χαρακτηριστικά:

- **Συμμετέχοντες:** 2 Παίκτες (Row Player vs Column Player)
- **Τύπος Παιγνίου:** Μηδενικού αθροίσματος (Zero-Sum), όπου $u_{row} + u_{col} = 0$
- **Διάρκεια:** Το πείραμα εκτελείται για $T = 5000$ επεισόδια
- **Πληροφόρηση:** Οι παίκτες επιλέγουν ενέργειες ταυτόχρονα

Το σύνολο των διαθέσιμων ενεργειών για κάθε παίκτη είναι:

$$
A = \{R, P, S\}
$$

Η μήτρα απολαβών (payoff matrix) για τον παίκτη Row ($A_{row}$) ορίζεται ως:

$$
A =
\begin{pmatrix}
0 & -1 & 1 \\
1 & 0 & -1 \\
-1 & 1 & 0
\end{pmatrix}
$$

Αντίστοιχα, για τον παίκτη Column ισχύει:

$$
u_{col} = -u_{row}
$$

---

## 2. Θεωρητικό Υπόβαθρο

### 2.1 Ισορροπία Nash (Nash Equilibrium)

Στο RPS, δεν υπάρχει καθαρή στρατηγική που να αποτελεί ισορροπία Nash. Η μοναδική **Μικτή Ισορροπία Nash** επιτυγχάνεται όταν οι παίκτες επιλέγουν τις ενέργειές τους με ομοιόμορφη κατανομή πιθανότητας:

$$
\pi^* = \left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right)
$$

Στην ισορροπία αυτή, η αναμενόμενη απόδοση (expected payoff) είναι μηδενική:

$$
E[u] = \pi_{row}^T A \pi_{col} = 0
$$

### 2.2 Αλγόριθμος Fictitious Play

Η μέθοδος Fictitious Play είναι ένας επαναληπτικός αλγόριθμος μάθησης. Σε κάθε γύρο, κάθε παίκτης:

1.  **Υπολογίζει την εμπειρική κατανομή** (belief) των επιλογών του αντιπάλου με βάση το ιστορικό:

$$
\hat{p}_{-i}(a) = \frac{N_{-i}(a)}{\sum_{a'} N_{-i}(a')}
$$

όπου $N_{-i}(a)$ είναι το πλήθος των φορών που ο αντίπαλος επέλεξε την ενέργεια $a$.

2.  **Επιλέγει τη Βέλτιστη Απόκριση (Best Response)** που μεγιστοποιεί την αναμενόμενη απόδοσή του ενάντια στην εκτιμώμενη στρατηγική του αντιπάλου.

---

## 3. Μεθοδολογία Αξιολόγησης

Για την αξιολόγηση της σύγκλισης και της απόδοσης των πρακτόρων χρησιμοποιήθηκαν οι εξής μετρικές:

### 3.1 Απόσταση από την Ισορροπία Nash (L1 Distance)

Μετράται η απόσταση Manhattan (L1 norm) των εμπειρικών στρατηγικών των παικτών από τη θεωρητική ισορροπία Nash $\pi^*$. Στόχος είναι η ελαχιστοποίηση της απόστασης $d$:

$$
d =
\left\| \hat{\pi}_{row} - \pi^{*}_{L_1} \right\|_{1}
+
\left\| \hat{\pi}_{col} - \pi^{*}_{L_1} \right\|_{1}
$$

όπου $\|p-q\|_1 = \sum_i |p_i - q_i|$.

### 3.2 Exploitability (Εκμεταλλευσιμότητα)

Η Exploitability μετρά πόσο μπορεί να κερδίσει ένας βέλτιστος αντίπαλος εναντίον της τρέχουσας στρατηγικής των παικτών.
Έστω $V = \pi_{row}^T A \pi_{col}$ η τρέχουσα αξία του παιχνιδιού.

- **Row Best Response:** $BR_{row} = \max_a (A \pi_{col})_a$
- **Column Best Response:** $Min_{col} = \min_b (\pi_{row}^T A)_b$

Το συνολικό exploitability ορίζεται ως:

$$
Exploit = (BR_{row} - V) + (V - Min_{col})
$$

Στην ισορροπία Nash, ισχύει $Exploit = 0$.

### 3.3 Μέση Απόδοση (Average Payoff)

Υπολογίζεται ο κινούμενος μέσος όρος (Moving Average - MA) σε παράθυρο $W$:

$$
MA_t = \frac{1}{W} \sum_{k=t-W+1}^{t} r_k
$$

Στο RPS, αναμένουμε ο $MA_t$ να συγκλίνει στο 0.

---

## 4. Πειραματικά Αποτελέσματα & Συζήτηση

Από την εκτέλεση του πειράματος για 5000 επεισόδια προέκυψαν τα εξής συμπεράσματα:

1.  **Σύγκλιση Εμπειρικών Στρατηγικών:** Παρόλο που οι άμεσες ενέργειες των παικτών παρουσιάζουν κυκλικότητα (ταλαντώσεις), οι εμπειρικές κατανομές συγκλίνουν σταθερά στο $(1/3, 1/3, 1/3)$.
2.  **Μείωση της Απόστασης Nash:** Η μετρική L1 Distance παρουσιάζει φθίνουσα πορεία, επιβεβαιώνοντας τη θεωρητική πρόβλεψη για το Fictitious Play.
3.  **Μηδενισμός Exploitability:** Η Exploitability τείνει ασυμπτωτικά στο μηδέν, υποδεικνύοντας ότι οι στρατηγικές γίνονται λιγότερο ευάλωτες με την πάροδο του χρόνου.
4.  **Μέσες Απολαβές:** Ο κινούμενος μέσος όρος των απολαβών συγκλίνει στο 0.

---

## 5. Συμπεράσματα

Η εφαρμογή του Fictitious Play στο Rock–Paper–Scissors επιβεβαιώνει ότι σε παίγνια μηδενικού αθροίσματος, ο αλγόριθμος προσεγγίζει επιτυχώς τη μικτή ισορροπία Nash. Οι παίκτες, μαθαίνοντας μέσα από την αλληλεπίδραση, υιοθετούν μακροπρόθεσμα τη βέλτιστη τυχαία στρατηγική, εξαλείφοντας το πλεονέκτημα του αντιπάλου.

# Μελέτη της Δυναμικής Μάθησης Ανεξάρτητων Πρακτόρων (Independent Q-Learning) στο Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου

## 1. Εισαγωγή και Περιγραφή Προβλήματος

Στην παρούσα ενότητα εξετάζεται η συμπεριφορά δύο πρακτόρων Ενισχυτικής Μάθησης (Reinforcement Learning - RL) που αλληλεπιδρούν σε ένα περιβάλλον **Επαναλαμβανόμενου Διλήμματος του Φυλακισμένου (Repeated Prisoner’s Dilemma - RPD)**.

Το πρόβλημα χαρακτηρίζεται από τα εξής:
- **Πράκτορες:** 2 ανεξάρτητοι μάθοι (Independent Learners), οι οποίοι δεν έχουν άμεση γνώση της πολιτικής του αντιπάλου, αλλά τον αντιμετωπίζουν ως μέρος ενός μη-στατικού περιβάλλοντος.
- **Διάρκεια:** Το πείραμα εκτελείται για $50.000$ επεισόδια, με ορίζοντα $200$ βημάτων ανά επεισόδιο.
- **Χώρος Καταστάσεων:** Η κατάσταση $s_t$ ορίζεται από την κοινή δράση του προηγούμενου γύρου: $s_t \in \{CC, CD, DC, DD\}$.
- **Χώρος Δράσεων:** $A = \{C, D\}$, όπου $C$ η Συνεργασία (Cooperate) και $D$ η Προδοσία (Defect).

Στόχος είναι η διερεύνηση της σύγκλισης του συστήματος: αν οδηγείται στο Nash Equilibrium $(D,D)$, σε αμοιβαία συνεργασία $(C,C)$, ή σε ταλαντωτική συμπεριφορά.

---

## 2. Θεωρητικό Υπόβαθρο

### 2.1 Το Δίλημμα του Φυλακισμένου

Ο πίνακας αποδόσεων (payoff matrix) του παιγνίου ορίζεται ως εξής:

|        | **C** | **D** |
|--------|----------|----------|
| **C** | $(3,3)$  | $(0,5)$  |
| **D** | $(5,0)$  | $(-1,-1)$|

Η στρατηγική $D$ είναι αυστηρά κυρίαρχη (strictly dominant), καθιστώντας το προφίλ $(D,D)$ το μοναδικό **Ισορροπία Nash (Nash Equilibrium)**, παρόλο που το $(C,C)$ είναι Pareto optimal.

### 2.2 Αλγόριθμος Independent Q-Learning (IQL)

Κάθε πράκτορας διατηρεί έναν πίνακα τιμών $Q(s,a)$ και τον ενημερώνει ανεξάρτητα, αγνοώντας τη διαδικασία μάθησης του αντιπάλου. Ο κανόνας ενημέρωσης είναι:

$$
Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') \right]
$$

**Παράμετροι Μάθησης:**
- **Learning Rate:** $\alpha = 0.1$
- **Discount Factor:** $\gamma = 0.95$ (ενθαρρύνει μακροπρόθεσμες στρατηγικές)

**Πολιτική Εξερεύνησης ($\varepsilon$-greedy):**
Η πιθανότητα τυχαίας δράσης ($\varepsilon$) μειώνεται γραμμικά από $\varepsilon_{start} = 1.0$ σε $\varepsilon_{end} = 0.03$ κατά τα πρώτα $20.000$ επεισόδια. Στη συνέχεια παραμένει σταθερή στο $0.03$ (97% greedy policy).

---

## 3. Μεθοδολογία Αξιολόγησης

Για την ανάλυση της δυναμικής του συστήματος χρησιμοποιούνται οι εξής μετρικές (υπολογισμένες με κινούμενο παράθυρο 200 επεισοδίων):

### 3.1 Απόδοση και Στρατηγική
- **Rolling Mean Episode Return:** Η μέση συνολική ανταμοιβή ανά επεισόδιο.
- **Πιθανότητα Συνεργασίας $P(C)$:** Η συχνότητα επιλογής της δράσης $C$.
- **Joint Action Occupancy:** Το ποσοστό εμφάνισης των καταστάσεων $(C,C), (D,D), (C,D), (D,C)$.

### 3.2 Σταθερότητα (Stability)
- **Action Switch Rate:** Η συχνότητα αλλαγής δράσης εντός του επεισοδίου.
- **L1 Occupancy Change:** Ο ρυθμός μεταβολής της κατανομής των καταστάσεων:
  $$L1_t = \sum_i | occ_t^i - occ_{t-1}^i |$$
- **Rolling Reward Variance:** Η διακύμανση των ανταμοιβών στο παράθυρο παρατήρησης:
  $$Var(R) = \frac{1}{N} \sum_{i=1}^{N} (R_i - \bar{R})^2$$

---

## 4. Πειραματικά Αποτελέσματα

Η ανάλυση των αποτελεσμάτων διακρίνεται σε δύο φάσεις:

### Φάση 1: Εκμάθηση (0 – 20.000 επεισόδια)
Κατά τη διάρκεια της μείωσης του $\varepsilon$:
- Η πιθανότητα συνεργασίας μειώνεται σταδιακά.
- Το σύστημα τείνει προς το Nash Equilibrium $(D,D)$.
- Οι αποδόσεις των πρακτόρων μειώνονται, προσεγγίζοντας την τιμή της αμοιβαίας προδοσίας.

### Φάση 2: Ώριμη Λειτουργία (> 20.000 επεισόδια)
Με σταθερό $\varepsilon = 0.03$, παρατηρούνται τα εξής φαινόμενα:
- **Αστάθεια:** Δεν επιτυγχάνεται σταθερή σύγκλιση.
- **Ταλαντώσεις:** Εμφανίζονται κύκλοι όπου το σύστημα μεταβαίνει από το $(D,D)$ προσπάθειες συνεργασίας $(C,C)$ και πίσω, λόγω της προσπάθειας των πρακτόρων να μεγιστοποιήσουν το ατομικό κέρδος σε ένα περιβάλλον που αντιλαμβάνονται λανθασμένα ως στατικό.
- **Υψηλή Διακύμανση:** Το switch rate και το variance των ανταμοιβών παραμένουν υψηλά.

---

## 5. Συμπεράσματα

Η εφαρμογή του Independent Q-Learning στο Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου οδηγεί στα εξής συμπεράσματα:

1.  **Απουσία Σύγκλισης Nash:** Το σύστημα δεν καταλήγει σε σταθερή ισορροπία Nash $(D,D)$, ούτε σε μόνιμη συνεργασία.
2.  **Μη-Στατικότητα:** Η ταυτόχρονη μάθηση παραβιάζει την υπόθεση στατικότητας του Q-Learning (Markov Property), οδηγώντας σε κυκλική δυναμική.
3.  **Ευαισθησία στο Θόρυβο:** Ακόμη και ελάχιστη εξερεύνηση ($\varepsilon = 0.03$) είναι αρκετή για να διαταράξει μια προσωρινή ισορροπία και να προκαλέσει μετάβαση σε άλλο καθεστώς συμπεριφοράς (regime switching).

Συνοψίζοντας, η πολυπρακτορική μάθηση με ανεξάρτητους Q-Learners σε κοινωνικά διλήμματα τείνει να παράγει δυναμικές συμπεριφορές αντί για στατικές ισορροπίες.

# Συγκριτική Μελέτη Independent Q-Learning και Minimax Q-Learning σε Παίγνια Μηδενικού Αθροίσματος: Η Περίπτωση του Μετασχηματισμένου Διλήμματος του Φυλακισμένου

## 1. Εισαγωγή και Διατύπωση Προβλήματος

Στην παρούσα εργασία διερευνάται η διαδικασία μάθησης ισορροπίας σε ένα επαναλαμβανόμενο παίγνιο δύο παικτών. Συγκεκριμένα, συγκρίνονται δύο διαφορετικές προσεγγίσεις Πολυπρακτορικής Ενισχυτικής Μάθησης (Multi-Agent Reinforcement Learning - MARL):
1.  **Independent Q-Learning (IQL):** Μια "αφελής" προσέγγιση που αγνοεί την ύπαρξη του αντιπάλου.
2.  **Minimax Q-Learning:** Μια προσέγγιση ειδικά σχεδιασμένη για ανταγωνιστικά παίγνια μηδενικού αθροίσματος (zero-sum).

Το πεδίο δοκιμής είναι το **Επαναλαμβανόμενο Δίλημμα του Φυλακισμένου (Repeated Prisoner’s Dilemma)**, το οποίο όμως έχει μετατραπεί τεχνητά σε παίγνιο μηδενικού αθροίσματος μέσω της τεχνικής **Reward Shaping**.

### Χαρακτηριστικά Περιβάλλοντος
- **Διάρκεια:** $50.000$ επεισόδια με ορίζοντα $200$ βημάτων το καθένα.
- **Χώρος Δράσεων:** $A = \{C, D\}$ (Συνεργασία, Προδοσία).
- **Ορισμός Κατάστασης:** Η κατάσταση $s_t$ καθορίζεται από την κοινή δράση του προηγούμενου γύρου:
  $$s_t = (a_A^{t-1}, a_B^{t-1})$$
  γεγονός που δημιουργεί ένα στοχαστικό παίγνιο 4 καταστάσεων.

### Zero-Sum Reward Shaping
Για να καταστεί δυνατή η εφαρμογή του αλγορίθμου Minimax, οι απολαβές τροποποιούνται ώστε το κέρδος του ενός παίκτη να ισούται με τη ζημία του άλλου:

$$
r_A^{zs} = r_A - r_B
$$
$$
r_B^{zs} = -r_A^{zs} \implies r_A^{zs} + r_B^{zs} = 0
$$

Μέσω αυτού του μετασχηματισμού, παρόλο που το υποκείμενο παίγνιο παραμένει το Prisoner's Dilemma, ο στόχος βελτιστοποίησης γίνεται αμιγώς ανταγωνιστικός.

---

## 2. Αλγόριθμοι Μάθησης

### 2.1 Independent Q-Learning (IQL)
Ο πράκτορας IQL αντιμετωπίζει τον αντίπαλο ως μέρος του στατικού περιβάλλοντος. Μαθαίνει μια συνάρτηση αξίας $Q(s,a)$ χρησιμοποιώντας τον standard κανόνα ανανέωσης Q-Learning:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \Big( r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big)
$$

Χρησιμοποιείται πολιτική **$\varepsilon$-greedy** με γραμμική μείωση της εξερεύνησης ($\varepsilon$) από $1.0$ σε $0.03$ κατά τα πρώτα $20.000$ επεισόδια.
- $\alpha = 0.1$
- $\gamma = 0.95$

### 2.2 Minimax Q-Learning
Σε περιβάλλοντα μηδενικού αθροίσματος, ο πράκτορας οφείλει να λαμβάνει υπόψη ότι ο αντίπαλος θα προσπαθήσει να ελαχιστοποιήσει το κέρδος του. Η συνάρτηση αξίας εξαρτάται από τις δράσεις και των δύο παικτών $Q(s, a_{self}, a_{opp})$.

Η αξία της κατάστασης $V(s)$ ορίζεται βάσει της αρχής του **minimax**:
$$
V(s) = \max_{a} \min_{a'} Q(s, a, a')
$$

Ο κανόνας ανανέωσης ενσωματώνει αυτή την αξία:
$$
Q(s,a_i,a_j) \leftarrow Q(s,a_i,a_j) + \alpha \Big( r + \gamma V(s') - Q(s,a_i,a_j) \Big)
$$
Η πολιτική του πράκτορα επιλέγει τη δράση που μεγιστοποιεί το ελάχιστο προσδοκώμενο κέρδος (maximin strategy).

---

## 3. Μεθοδολογία Αξιολόγησης

Η σύγκριση των αλγορίθμων βασίζεται στις ακόλουθες μετρικές (με χρήση rolling window 200 επεισοδίων):

1.  **Rolling Joint-Action Occupancy:** Η συχνότητα εμφάνισης των ζευγών δράσεων $(C,C), (D,D), (C,D), (D,C)$.
2.  **Rolling Reward Variance:** Η διακύμανση των ανταμοιβών $Var(R)$, η οποία αποτελεί δείκτη σταθερότητας της στρατηγικής.
3.  **L1 Occupancy Change:** Ο ρυθμός μεταβολής της κατανομής των καταστάσεων, που υποδεικνύει την ταχύτητα σύγκλισης:
    $$L1_t = \sum_i \left| p_t(i) - p_{t-1}(i) \right|$$
4.  **Rolling Episode Return:** Το αθροιστικό shaped reward ανά επεισόδιο. Σε ισορροπία zero-sum, αναμένεται $E[R] \approx 0$.

---

## 4. Ανάλυση Αποτελεσμάτων

### 4.1 Σύγκλιση Στρατηγικής (Occupancy)
Παρατηρείται σαφής και μονοτονική αύξηση της κατάστασης **$(D,D)$**, η οποία κυριαρχεί πλήρως (ποσοστό $\approx 100\%$) μετά το πέρας των $20.000$ επεισοδίων (περίοδος μείωσης του $\varepsilon$).

**Ερμηνεία:**
Στον μετασχηματισμένο πίνακα απολαβών, η κατάσταση $(D,D)$ αποτελεί το σημείο ισορροπίας (saddle point) με τιμή 0. Ο Minimax πράκτορας οδηγεί το παιχνίδι σε αυτή την ασφαλή στρατηγική, και ο IQL πράκτορας αναγκάζεται να προσαρμοστεί (best response) παίζοντας επίσης $D$.

### 4.2 Σταθερότητα (Variance & L1 Change)
- **L1 Change:** Εμφανίζει υψηλές τιμές μόνο στην αρχή. Μετά το επεισόδιο $20.000$, η μεταβολή μηδενίζεται, υποδεικνύοντας ότι το σύστημα έχει "κλειδώσει" σε σταθερή συμπεριφορά.
- **Reward Variance:** Υψηλή διακύμανση κατά τη φάση εξερεύνησης, η οποία μειώνεται απότομα και σταθεροποιείται σε χαμηλά επίπεδα μόλις ολοκληρωθεί η σύγκλιση.

### 4.3 Απολαβές (Return)
Οι μέσες απολαβές παρουσιάζουν αρχικά έντονες διακυμάνσεις, αλλά τελικά συγκλίνουν στο $0$. Αυτό επιβεβαιώνει ότι έχει επιτευχθεί η ισορροπία του παιγνίου μηδενικού αθροίσματος (Value of the Game = 0).

---

## 5. Συμπεράσματα

Η πειραματική διαδικασία κατέδειξε τα εξής:

1.  **Επιτυχής Σύγκλιση:** Σε αντίθεση με την περίπτωση του απλού IQL vs IQL, η εισαγωγή του Minimax πράκτορα και η zero-sum δομή οδήγησαν σε ταχεία και σταθερή σύγκλιση.
2.  **Χρόνος Σύγκλισης:** Το σύστημα σταθεροποιείται περίπου στα $20.000$ επεισόδια, ταυτιζόμενο χρονικά με το τέλος της φάσης έντονης εξερεύνησης (exploration decay).
3.  **Κυριαρχία της Στρατηγικής Minimax:** Ο αλγόριθμος Minimax Q-Learning επέβαλε επιτυχώς τη στρατηγική ασφαλείας $(D,D)$, μετατρέποντας ένα κοινωνικό δίλημμα σε καθαρά ανταγωνιστική ισορροπία.